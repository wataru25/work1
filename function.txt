# -*- coding: utf-8 -*-
"""
Created on Mon Oct 25 10:12:57 2021

@author: finst
"""

import numpy as np
import os
import matplotlib.pyplot as plt
import csv
from statistics import mean, variance
from scipy import stats
import pandas as pd
from pandas import read_csv as rc
from sklearn.neighbors import KernelDensity
from sklearn.decomposition import PCA
from sklearn.decomposition import KernelPCA
from sklearn.metrics import explained_variance_score
from sklearn.decomposition import SparsePCA
from sklearn.decomposition import FastICA
from sklearn.ensemble import IsolationForest
#作業ディレクトリを指定
work_path = "C:/Users/finst/OneDrive/デスクトップ/wataru/13_仕事/運企予兆"

os.chdir(work_path)
#確率的主成分分析に必要な情報を読み込み
import PPCA

#==============================================================================
# 関数定義
#==============================================================================
# ホテリング
def hotelling(valid_data,judge_data,threshold):
    """
    ホテリング理論を用いて異常値判定を行う関数

    Parameters
    ----------
    valid_data : pandas_dataframe
        正常データ
    judge_data : pandas_dataframe
        判定データ
    threshold : real
        閾値

    Returns
    -------
    bool
        TRUEならば異常値，FALSEならば正常値

    """
    
    #平均と分散の算出
    Mean = valid_data.mean()
    Variance = valid_data.cov()
    
    #異常値の算出
    vector = judge_data - Mean
    vector = vector.values
    matrix = Variance.values
    matrix = np.linalg.inv(matrix)
    
    anomaly = np.dot(np.dot(vector.T,matrix),vector)
    
    #カイ2乗分布における1%水準の閾値
    threshold_anomaly = stats.chi2.interval(1-threshold,1)[1]
    
    if anomaly < threshold_anomaly:
        return [True,anomaly,threshold_anomaly]
    else :
        return [False,anomaly,threshold_anomaly]
    
    

# 近傍法
def distance(data1,data2,kind):
    """
    Parameters
    ----------
    data1 : pandas dataframe
    data2 : pandas dataframe
    kind  : string
        距離の種類を選択
    
    Returns
    -------
    Real number
    data1とdata2の距離を測定
    
    """
    data1 = data1.to_numpy()
    data2 = data2.to_numpy()
    
    if kind =="E":
        distance = sum((data1-data2)**2)
    elif kind == "m":
        distance = sum((data1-data2).abs())
    elif kind == "c":
        distance = max((data1-data2).abs())
                       
    return distance
      
def kNN(valid_data,judge_data,th_num,th_dis,kind):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    jude_data : pandas dataframe
        判定するデータ
    threshold : real
        距離の閾値
    kind : string
        距離の種類

    Returns
    -------
    bool
        Trueならば異常値，Falseならば正常値
    """
    
    row_num = len(valid_data)
    dist_list = np.array([distance(valid_data.iloc[i,:],judge_data,kind) for i in range(row_num)])
    count=np.count_nonzero(dist_list<th_dis)
    
    if count <= th_num :
        return [True,count,th_num]
    else:
        return [False,count,th_num]

def eNN(valid_data,judge_data,th_dis,th_num,kind):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    juded_data : pandas dataframe
        判定するデータ
    threshold : natural number
        個数のの閾値
    kind : string
        距離の種類

    Returns
    -------
    bool
        Trueならば異常値，Falseならば正常値
    """
    
    
    row_num = len(valid_data)
    all_distance = [distance(valid_data.iloc[i,:],judge_data,kind) for i in range(row_num)]
    dis = sorted(all_distance)[-th_num]
        
    if dis >= th_dis :
        return [True,dis,th_dis]
    else:
        return [False,dis,th_dis]


#カーネル密度推定法
def kernel_density(valid_data,judge_data,kernel_density,band_width,threshold):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    juded_data : pandas dataframe
        判定するデータ
    kernel_density : string
        使用するカーネル
    band_width :　実数
        バンド幅の初期値
    threshold : 実数
        異常度の閾値

    Returns
    -------
    bool
        Trueならば異常値，Falseならば正常値
    """
    # データの加工
    col_number = len(valid_data.columns)
    vlist = [np.array(valid_data.iloc[:,i]) for i in range(col_number)]
    vlist_tuple = tuple(vlist)
    X = np.vstack(vlist_tuple).T
    
    # モデルの定義
    # bwは自動で調整
    kde_model = KernelDensity(kernel=kernel_density).fit(X)
    # 異常度の計算
    anomaly = -kde_model.score_samples([np.array(judge_data)])[0]
    
    if anomaly > threshold:
        return [True,anomaly,threshold]
    else:
        return [False,anomaly,threshold]
    
#isolation_forest
def isolation_forest(valid_data,judge_data):
    """

    Parameters
    ----------
    valid_data : pandas_DataFrame
        正常データ
    judge_data : pandas_DataFrame
        判定したいデータ

    Returns
    -------
    bool
        Trueならば異常値，Falseならば正常値  
    """
    clf = IsolationForest()
    clf.fit(valid_data)

    anomaly = clf.predict([judge_data])[0]
    
    if anomaly == -1 :
        return [True]
    else :
        return [False]
    
    
def reduction_PCA(valid_data,judge_data,after_dim):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    judge_data : pandas dataframe
        異常値判定データ
    after_dim : 実数
        次元削減後の次元
    Returns
    -------
    valid_data
        次元削減後の訓練データ
    judge_data
        次元削減後の異常値判定データ
    ratio
        寄与率の合計
    score
        スコア合計
    """
    
    pca_model = PCA(n_components = after_dim)
    pca_model.fit(valid_data)
    after_data_valid = pd.DataFrame(pca_model.transform(valid_data))
    after_data_judge = pd.DataFrame(pca_model.transform([judge_data])).iloc[0]
    
    ratio = sum(pca_model.explained_variance_ratio_)
    score = explained_variance_score(valid_data, pca_model.inverse_transform(after_data_valid))
    
    return {"valid_data":after_data_valid,"judge_data":after_data_judge,"ratio":ratio,"score":score,"model":pca_model}

def reduction_KernelPCA(valid_data,judge_data,after_dim,kernel,gamma,degree=3,coef0 = 1):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    judge_data : pandas dataframe
        異常値判定データ
    after_dim : int
        次元削減後の次元
    kernel : string("linear","poly","rbf","sigmoid","cosine")
        使用するカーネル関数
    gamma : float
        rbfとpolyのカーネル係数
    degree : int
        polyの係数
    coef0 : float
        polyとsigmoidの独立係数
        
    Returns
    -------
    valid_data
        次元削減後の訓練データ
    judge_data
        次元削減後の異常値判定データ
    ratio
        寄与率の合計
    score
        スコア合計
    
    """
    
    kpca_model = KernelPCA(n_components = after_dim,kernel = kernel,gamma = gamma,degree= degree,coef0 = coef0, fit_inverse_transform=True)
    kpca_model.fit(valid_data)
    after_data_valid = pd.DataFrame(kpca_model.transform(valid_data))
    after_data_judge = pd.DataFrame(kpca_model.transform([judge_data])).iloc[0]
    
    #ratio = sum(np.var(after_data_valid,axis = 0)/np.sum(np.var(after_data_valid,axis = 0)))
    score = explained_variance_score(valid_data, kpca_model.inverse_transform(after_data_valid))
    
    return {"valid_data":after_data_valid,"judge_data":after_data_judge,"score":score,"model":kpca_model}

def reduction_SparsePCA(valid_data,judge_data,after_dim,alpha = 1):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    judge_data : pandas dataframe
        異常値判定データ
    after_dim : int
        次元削減後の次元
    alpha : float
        L1正則化の強さ
        
    Returns
    -------
    pandas dataframe
        次元削減後のデータ
    実数
        寄与率の合計
    """
    
    spca_model = SparsePCA(n_components = after_dim,alpha = alpha)
    spca_model.fit(valid_data)
    after_data_valid = pd.DataFrame(spca_model.transform(valid_data))
    after_data_judge = pd.DataFrame(spca_model.transform([judge_data])).iloc[0]
    
    #ratio = sum(spca_model.explained_variance_ratio_)
    #score = explained_variance_score(valid_data, spca_model.inverse_transform(after_data_valid))
    
    return {"valid_data":after_data_valid,"judge_data":after_data_judge,"model":spca_model}

def reduction_ProbablePCA(valid_data,judge_data,after_dim):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    judge_data : pandas dataframe
        異常値判定データ
    after_dim : int
        次元削減後の次元
        
    Returns
    -------
    pandas dataframe
        次元削減後のデータ
    実数
        寄与率の合計
    """
    valid_data_numpy = valid_data.values
    judge_data_numpy = judge_data.values
    
    ppca_model = PPCA.BPCA(m=after_dim)
    ppca_model.fit(valid_data_numpy)

    after_data_valid = pd.DataFrame(ppca_model.transform(valid_data_numpy))
    after_data_judge = pd.DataFrame(ppca_model.transform(judge_data_numpy)).iloc[:,0]
    
    #ratio = sum(spca_model.explained_variance_ratio_)
    #score = explained_variance_score(valid_data, spca_model.inverse_transform(after_data_valid))
    
    return {"valid_data":after_data_valid,"judge_data":after_data_judge,"model":ppca_model}

def reduction_ICA(valid_data,judge_data,after_dim):
    """
    Parameters
    ----------
    valid_data : pandas dataframe
        訓練データ
    judge_data : pandas dataframe
        異常値判定データ
    after_dim : int
        次元削減後の次元
    alpha : float
        L1正則化の強さ
        
    Returns
    -------
    pandas dataframe
        次元削減後のデータ
    実数
        寄与率の合計
    """
    
    ica_model = FastICA(n_components = after_dim)
    ica_model.fit(valid_data)
    after_data_valid = pd.DataFrame(ica_model.transform(valid_data))
    after_data_judge = pd.DataFrame(ica_model.transform([judge_data])).iloc[0]
    
    #ratio = sum(ica_model.explained_variance_ratio_)
    #score = explained_variance_score(valid_data, spca_model.inverse_transform(after_data_valid))
    
    return {"valid_data":after_data_valid,"judge_data":after_data_judge,"model":ica_model}

    
    